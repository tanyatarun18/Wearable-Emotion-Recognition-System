{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IWgUZRdtXdgq",
        "outputId": "511a5486-b64a-4d94-8a0c-7902674961c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Mounted at /content/drive\n",
            "Drive mounted successfully.\n",
            "Dataset path found: /content/drive/MyDrive/WESAD\n",
            "\n",
            "Found 2 subject data files.\n",
            "Example path: /content/drive/MyDrive/WESAD/S16/S16.pkl\n",
            "\n",
            "Processing data and creating windows for all subjects...\n",
            "  - Processing S16...\n",
            "    - Added 148 windows for S16\n",
            "  - Processing S8...\n",
            "    - Added 148 windows for S8\n",
            "\n",
            "Total windows created: 296\n",
            "Window shape (samples, features): (120, 6)\n",
            "Labels shape: (296,)\n",
            "Unique labels: [0 1 2]\n",
            "\n",
            "Scaling data...\n",
            "Splitting into training and testing sets...\n",
            "\n",
            "Training data shape: (236, 120, 6)\n",
            "Testing data shape: (60, 120, 6)\n",
            "\n",
            "Building CNN-LSTM model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │         \u001b[38;5;34m1,984\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m98,816\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)              │           \u001b[38;5;34m195\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,984</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ maxpool_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,816</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">195</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m109,251\u001b[0m (426.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,251</span> (426.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m109,251\u001b[0m (426.76 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">109,251</span> (426.76 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compiling and training model...\n",
            "Epoch 1/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - accuracy: 0.5889 - loss: 0.9668 - val_accuracy: 0.7500 - val_loss: 0.5743\n",
            "Epoch 2/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 88ms/step - accuracy: 0.8240 - loss: 0.4543 - val_accuracy: 0.8667 - val_loss: 0.2192\n",
            "Epoch 3/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 151ms/step - accuracy: 0.9188 - loss: 0.1572 - val_accuracy: 0.9833 - val_loss: 0.1077\n",
            "Epoch 4/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 147ms/step - accuracy: 0.9802 - loss: 0.1275 - val_accuracy: 0.9500 - val_loss: 0.1548\n",
            "Epoch 5/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 154ms/step - accuracy: 0.9532 - loss: 0.1452 - val_accuracy: 0.9667 - val_loss: 0.0773\n",
            "Epoch 6/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 98ms/step - accuracy: 0.9841 - loss: 0.0534 - val_accuracy: 0.9667 - val_loss: 0.0882\n",
            "Epoch 7/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 0.9729 - loss: 0.0720 - val_accuracy: 0.9667 - val_loss: 0.0562\n",
            "Epoch 8/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 84ms/step - accuracy: 0.9973 - loss: 0.0295 - val_accuracy: 1.0000 - val_loss: 0.0217\n",
            "Epoch 9/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 86ms/step - accuracy: 0.9964 - loss: 0.0191 - val_accuracy: 1.0000 - val_loss: 0.0148\n",
            "Epoch 10/10\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 0.0068 - val_accuracy: 0.9833 - val_loss: 0.0575\n",
            "\n",
            "Model training complete.\n",
            "\n",
            "Test Accuracy: 98.33%\n",
            "\n",
            "Converting model to TensorFlow Lite format (with Select TF Ops for LSTM)...\n",
            "Saved artifact at '/tmp/tmpnr00tvy9'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 120, 6), dtype=tf.float32, name='input_layer')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 3), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  136588484723536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588484731600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588353519888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588353522000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588353522576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588353520848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588353522384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588353523344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  136588353523536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "Model converted successfully.\n",
            "\n",
            "--- SUCCESS! ---\n",
            "Model saved successfully to your Google Drive at:\n",
            "/content/drive/MyDrive/WESAD/emotion_model.tflite\n",
            "File size: 135.73 KB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import mode\n",
        "from google.colab import drive\n",
        "from scipy.signal import resample  # <-- ADDED IMPORT FOR RESAMPLING\n",
        "\n",
        "# ---\n",
        "# STEP 1: MOUNT GOOGLE DRIVE\n",
        "# ---\n",
        "print(\"Mounting Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting drive: {e}\")\n",
        "\n",
        "# ---\n",
        "# STEP 2: SET THE PATH TO YOUR DATASET FOLDER\n",
        "# ---\n",
        "# This is the most important step for you to edit.\n",
        "# Use the Colab file browser (on the left) to find your WESAD folder,\n",
        "# right-click it, and select \"Copy path\".\n",
        "# Paste that path into the variable below.\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# ----> !! IMPORTANT !! <----\n",
        "# ----> PLEASE EDIT THIS LINE with the correct file path:\n",
        "#\n",
        "# Example path: '/content/drive/MyDrive/WESAD'\n",
        "#\n",
        "dataset_drive_path = '/content/drive/MyDrive/WESAD'  # <-- Change this path!\n",
        "# -----------------------------------------------------------------------\n",
        "\n",
        "# Check if the path exists\n",
        "if not os.path.exists(dataset_drive_path) or 'https:' in dataset_drive_path:\n",
        "    print(f\"--- ERROR ---\")\n",
        "    if 'https:' in dataset_drive_path:\n",
        "        print(\"The path looks like a URL. It must be a file path.\")\n",
        "        print(\"Please use the file browser on the left, right-click your folder, and 'Copy path'.\")\n",
        "    else:\n",
        "        print(f\"The path '{dataset_drive_path}' does not exist.\")\n",
        "    print(\"Please check the path and try again.\")\n",
        "else:\n",
        "    print(f\"Dataset path found: {dataset_drive_path}\")\n",
        "\n",
        "# ---\n",
        "# STEP 3: FIND ALL INDIVIDUAL SUBJECT FILES\n",
        "# ---\n",
        "# This code will search inside your 'dataset_drive_path'\n",
        "# for all the subject .pkl files.\n",
        "\n",
        "# Use glob to recursively find all files ending in .pkl\n",
        "search_pattern = os.path.join(dataset_drive_path, '**', 'S*.pkl')\n",
        "subject_pkl_files = glob.glob(search_pattern, recursive=True)\n",
        "\n",
        "# IMPORTANT: Filter out the E4 (chest) .pkl files. We only want the main subject file.\n",
        "final_subject_files = []\n",
        "for pkl_path in subject_pkl_files:\n",
        "    file_name = os.path.basename(pkl_path)\n",
        "    parent_folder_name = os.path.basename(os.path.dirname(pkl_path))\n",
        "\n",
        "    # If S2.pkl is in folder S2, it's the one we want\n",
        "    if file_name.replace('.pkl', '') == parent_folder_name:\n",
        "        final_subject_files.append(pkl_path)\n",
        "\n",
        "if not final_subject_files:\n",
        "    print(f\"\\n--- ERROR ---\")\n",
        "    print(f\"Could not find any subject .pkl files (e.g., S2/S2.pkl) in the folder:\")\n",
        "    print(f\"{dataset_drive_path}\")\n",
        "    print(f\"Please check your 'dataset_drive_path' variable. Make sure it points to the folder containing S2, S3, etc.\")\n",
        "else:\n",
        "    print(f\"\\nFound {len(final_subject_files)} subject data files.\")\n",
        "    print(\"Example path:\", final_subject_files[0])\n",
        "\n",
        "# ---\n",
        "# STEP 4: DEFINE PRE-PROCESSING FUNCTIONS (REWRITTEN)\n",
        "# ---\n",
        "# This function is now corrected to resample all signals to a\n",
        "# common frequency (4Hz) before combining them.\n",
        "\n",
        "def process_subject_data(subject_data):\n",
        "    \"\"\"Extracts, resamples, and combines wrist data for one subject.\"\"\"\n",
        "\n",
        "    # Check if 'wrist' data is present\n",
        "    if 'wrist' not in subject_data['signal']:\n",
        "        return None, None\n",
        "\n",
        "    # Get the 4Hz signals (our target frequency)\n",
        "    try:\n",
        "        wrist_eda = subject_data['signal']['wrist']['EDA'].flatten()\n",
        "        wrist_temp = subject_data['signal']['wrist']['TEMP'].flatten()\n",
        "\n",
        "        # Use EDA as the reference length (it's already @ 4Hz)\n",
        "        target_len = len(wrist_eda)\n",
        "        if target_len == 0:\n",
        "            return None, None # Skip if no EDA data\n",
        "\n",
        "        # Get other signals (which are at different frequencies)\n",
        "        wrist_acc = np.array(subject_data['signal']['wrist']['ACC'])\n",
        "        wrist_bvp = np.array(subject_data['signal']['wrist']['BVP']).flatten()\n",
        "        labels_raw = np.array(subject_data['label']).flatten()\n",
        "\n",
        "        # Resample all other signals down to the target_len (N @ 4Hz)\n",
        "        acc_resampled = resample(wrist_acc, target_len)\n",
        "        bvp_resampled = resample(wrist_bvp, target_len)\n",
        "        temp_resampled = resample(wrist_temp, target_len) # Resample temp just in case length is slightly off\n",
        "\n",
        "        # For labels, we resample and then round to the nearest integer\n",
        "        # to preserve the categorical labels (1, 2, 3...).\n",
        "        labels_resampled = resample(labels_raw, target_len)\n",
        "        labels_rounded = np.round(labels_resampled).astype(int)\n",
        "\n",
        "        # Combine all features into a single array\n",
        "        features = np.hstack([\n",
        "            acc_resampled,                         # (N, 3)\n",
        "            bvp_resampled.reshape(-1, 1),          # (N, 1)\n",
        "            wrist_eda.reshape(-1, 1),              # (N, 1)\n",
        "            temp_resampled.reshape(-1, 1)          # (N, 1)\n",
        "        ])\n",
        "\n",
        "        labels = labels_rounded.flatten() # Ensure it's (N,)\n",
        "\n",
        "        return features, labels\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    - Error processing signals: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def create_windows(data, labels, window_size_sec, overlap_sec, fs=4):\n",
        "    \"\"\"Creates overlapping windows of data and assigns a label to each.\"\"\"\n",
        "\n",
        "    window_size = window_size_sec * fs  # 30 seconds * 4 Hz = 120 samples\n",
        "    overlap = overlap_sec * fs          # 15 seconds * 4 Hz = 60 samples\n",
        "    stride = window_size - overlap\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for i in range(0, len(data) - window_size, stride):\n",
        "        window_data = data[i : i + window_size]\n",
        "        window_labels = labels[i : i + window_size]\n",
        "\n",
        "        # Assign a single label to the window (most frequent label)\n",
        "        label = mode(window_labels, keepdims=True)[0]\n",
        "\n",
        "        # We are interested in 1 (Baseline), 2 (Stress), 3 (Amusement)\n",
        "        # We ignore 0 (transient) and other labels (4, 5, 6, 7)\n",
        "        if label in [1, 2, 3]:\n",
        "            X.append(window_data)\n",
        "            y.append(label[0]) # [0] to get the scalar value\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# ---\n",
        "# STEP 5: CREATE THE FULL, WINDOWED DATASET\n",
        "# ---\n",
        "\n",
        "all_X = []\n",
        "all_y = []\n",
        "\n",
        "# Define window parameters\n",
        "WINDOW_SEC = 30\n",
        "OVERLAP_SEC = 15\n",
        "SAMPLING_RATE = 4 # 4 Hz\n",
        "\n",
        "print(\"\\nProcessing data and creating windows for all subjects...\")\n",
        "\n",
        "for pkl_file_path in final_subject_files:\n",
        "    subject_id = os.path.basename(pkl_file_path).replace('.pkl', '')\n",
        "    print(f\"  - Processing {subject_id}...\")\n",
        "\n",
        "    try:\n",
        "        # Load the individual subject pickle file from Google Drive\n",
        "        with open(pkl_file_path, 'rb') as f:\n",
        "            subject_data = pickle.load(f, encoding='latin1')\n",
        "\n",
        "        features, labels = process_subject_data(subject_data)\n",
        "\n",
        "        if features is None or labels is None:\n",
        "            print(f\"    - SKIPPING {subject_id} (Missing wrist data or other error)\")\n",
        "            continue\n",
        "\n",
        "        # Create windows for this subject\n",
        "        X_subject, y_subject = create_windows(\n",
        "            features, labels, WINDOW_SEC, OVERLAP_SEC, SAMPLING_RATE\n",
        "        )\n",
        "\n",
        "        if X_subject.shape[0] > 0:\n",
        "            all_X.append(X_subject)\n",
        "            all_y.append(y_subject)\n",
        "            print(f\"    - Added {X_subject.shape[0]} windows for {subject_id}\")\n",
        "        else:\n",
        "            print(f\"    - No valid windows (labels 1, 2, 3) found for {subject_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"    - FAILED to process {subject_id}. Error: {e}\")\n",
        "\n",
        "\n",
        "# --- ADDED ERROR CHECK ---\n",
        "# Check if the lists are empty *before* trying to concatenate\n",
        "if not all_X or not all_y:\n",
        "    print(\"\\n\\n--- CRITICAL ERROR ---\")\n",
        "    print(\"No data was successfully processed.\")\n",
        "    print(\"This could mean your 'dataset_drive_path' (STEP 2) is wrong,\")\n",
        "    print(\"or the subjects found do not contain valid wrist data or labels (1, 2, 3).\")\n",
        "    print(\"Please check your path and the data.\")\n",
        "    raise ValueError(\"No valid subject data found to create windows.\")\n",
        "# --- END OF ADDED CHECK ---\n",
        "\n",
        "\n",
        "# Combine data from all subjects\n",
        "X = np.concatenate(all_X, axis=0)\n",
        "y = np.concatenate(all_y, axis=0)\n",
        "\n",
        "# We must remap labels from [1, 2, 3] to [0, 1, 2] for the AI model\n",
        "# 1 (Baseline) -> 0\n",
        "# 2 (Stress)    -> 1\n",
        "# 3 (Amusement) -> 2\n",
        "y = y - 1\n",
        "\n",
        "print(f\"\\nTotal windows created: {X.shape[0]}\")\n",
        "print(f\"Window shape (samples, features): {X.shape[1:]}\")\n",
        "print(f\"Labels shape: {y.shape}\")\n",
        "print(f\"Unique labels: {np.unique(y)}\")\n",
        "\n",
        "# ---\n",
        "# STEP 6: SCALE DATA & SPLIT FOR TRAINING\n",
        "# ---\n",
        "\n",
        "print(\"\\nScaling data...\")\n",
        "# We must scale the data\n",
        "# We scale each feature across all windows\n",
        "# X shape is (num_windows, num_samples, num_features)\n",
        "# We reshape to (num_windows * num_samples, num_features) to scale\n",
        "num_windows, num_samples, num_features = X.shape\n",
        "X_reshaped = X.reshape(-1, num_features)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled_reshaped = scaler.fit_transform(X_reshaped)\n",
        "\n",
        "# Reshape back to (num_windows, num_samples, num_features)\n",
        "X_scaled = X_scaled_reshaped.reshape(num_windows, num_samples, num_features)\n",
        "\n",
        "print(\"Splitting into training and testing sets...\")\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
        "print(f\"Testing data shape: {X_test.shape}\")\n",
        "\n",
        "# ---\n",
        "# STEP 7: BUILD THE CNN-LSTM MODEL (as per FIGURE 1)\n",
        "# ---\n",
        "\n",
        "# Model constants\n",
        "window_length = X_train.shape[1] # e.g., 120 samples\n",
        "num_features = X_train.shape[2]  # e.g., 6 features\n",
        "num_classes = 3                  # (0: Baseline, 1: Stress, 2: Amusement)\n",
        "\n",
        "print(\"\\nBuilding CNN-LSTM model...\")\n",
        "\n",
        "model = Sequential([\n",
        "    # Input layer\n",
        "    Input(shape=(window_length, num_features), name=\"input_layer\"),\n",
        "\n",
        "    # 1. CNN Feature Extraction\n",
        "    Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', name=\"conv1d_1\"),\n",
        "    MaxPooling1D(pool_size=2, name=\"maxpool_1\"),\n",
        "\n",
        "    # 2. LSTM Temporal Analysis\n",
        "    LSTM(128, return_sequences=False, name=\"lstm_1\"),\n",
        "\n",
        "    # 3. Output Layer (Emotion Classification)\n",
        "    Dense(64, activation='relu', name=\"dense_1\"),\n",
        "    Dense(num_classes, activation='softmax', name=\"output_layer\")\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ---\n",
        "# STEP 8: TRAIN THE MODEL\n",
        "# ---\n",
        "\n",
        "print(\"\\nCompiling and training model...\")\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='sparse_categorical_crossentropy', # Use this for integer labels\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train for a few epochs\n",
        "# In a real project, you would train for 50-100 epochs\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test)\n",
        ")\n",
        "\n",
        "print(\"\\nModel training complete.\")\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# ---\n",
        "# ---\n",
        "# STEP 9: CONVERT & SAVE THE TFLite MODEL (Corrected)\n",
        "# ---\n",
        "# This step adds the required flags to handle the LSTM layer conversion.\n",
        "\n",
        "print(\"\\nConverting model to TensorFlow Lite format (with Select TF Ops for LSTM)...\")\n",
        "\n",
        "# Convert the Keras model\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "# --- THIS IS THE FIX ---\n",
        "# 1. Tell the converter to allow TensorFlow ops (for the LSTM)\n",
        "converter.target_spec.supported_ops = [\n",
        "    tf.lite.OpsSet.TFLITE_BUILTINS,  # Enable default TFLite ops\n",
        "    tf.lite.OpsSet.SELECT_TF_OPS     # Enable TensorFlow ops (for the LSTM)\n",
        "]\n",
        "# 2. Disable the experimental feature that causes the error\n",
        "converter._experimental_lower_tensor_list_ops = False\n",
        "# --- END OF FIX ---\n",
        "\n",
        "# We can still apply the default optimizations\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "try:\n",
        "    # Try to convert the model with the new settings\n",
        "    tflite_model = converter.convert()\n",
        "    print(\"Model converted successfully.\")\n",
        "\n",
        "    # Define the output path\n",
        "    # We'll save it inside the same folder you specified in STEP 2\n",
        "    output_filename = os.path.join(dataset_drive_path, 'emotion_model.tflite')\n",
        "\n",
        "    # Save the .tflite file to your Google Drive\n",
        "    try:\n",
        "        with open(output_filename, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "\n",
        "        print(f\"\\n--- SUCCESS! ---\")\n",
        "        print(f\"Model saved successfully to your Google Drive at:\")\n",
        "        print(output_filename)\n",
        "        print(f\"File size: {os.path.getsize(output_filename) / 1024:.2f} KB\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n--- ERROR SAVING FILE ---\")\n",
        "        print(f\"Could not save model to {output_filename}\")\n",
        "        print(f\"Error: {e}\")\n",
        "        print(\"Trying to save locally instead...\")\n",
        "        # Fallback to saving locally in Colab\n",
        "        local_filename = 'emotion_model.tflite'\n",
        "        with open(local_filename, 'wb') as f:\n",
        "            f.write(tflite_model)\n",
        "        print(f\"Successfully saved model locally as '{local_filename}'\")\n",
        "        print(\"You will need to download it manually from the Colab file browser.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- TFLITE CONVERSION FAILED ---\")\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"This can happen if TensorFlow versions are incompatible. Please check the runtime.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qBslGN72jX9w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}